{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad3b652",
   "metadata": {},
   "source": [
    "# **PhoBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d63759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "access_token = \"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\"\n",
    "login(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc8e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\envs\\QandA\\lib\\site-packages\\keras\\losses.py:2674: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbccc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset\")\n",
    "VECTORIZER_DIR = os.path.join(PROJECT_DIR, \"vectorizer\")\n",
    "MODEL_DIR = \"D:/VnCoreNLP\"\n",
    "corpus_path = os.path.join(PROJECT_DIR, \"dataset\", \"processed_legal_corpus.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b0b9d",
   "metadata": {},
   "source": [
    "### **Create sentence encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d65c9d",
   "metadata": {},
   "source": [
    "With encoder from pretrain model PhoBERT\n",
    "\n",
    "Mean pooling to get the sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 1: Load encoder (PhoBERT)\n",
    "phobert = models.Transformer(\"vinai/phobert-base-v2\", max_seq_length=128)\n",
    "\n",
    "# Bước 2: Thêm pooling (mean pooling để lấy sentence embedding)\n",
    "pooling = models.Pooling(\n",
    "    word_embedding_dimension=phobert.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "# Bước 3: Kết hợp thành mô hình sentence embedding\n",
    "model = SentenceTransformer(modules=[phobert, pooling])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab444f07",
   "metadata": {},
   "source": [
    "### **Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open train set\n",
    "data_path = os.path.join(DATASET_DIR, \"processed_train_data.json\")\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7485948",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "n = len(data)\n",
    "train_num = int((n/10)*8)\n",
    "test_num = n - train_num\n",
    "\n",
    "for i in range(train_num):\n",
    "    train_data.append(InputExample(texts=data[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=2)\n",
    "\n",
    "print(train_num)\n",
    "print(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu\n",
    "model.save(\"phobert-sentence-embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045d321",
   "metadata": {},
   "source": [
    "### **Load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60315d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 4.1.0, however, your version is 3.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load lại để sử dụng\n",
    "sentence_embedded_model = SentenceTransformer(os.path.join(VECTORIZER_DIR, \"phobert\"), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62303308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedded_model.device)  # -> cuda:0 hoặc cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3c6a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61425\n",
      "61425\n"
     ]
    }
   ],
   "source": [
    "# READ THE CORPUS TO get the content\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Danh sách văn bản\n",
    "content = []\n",
    "index = [] # \"01/2009/tt-bnn 1\" (law_id + article_id)\n",
    "\n",
    "for document in data:\n",
    "    law_id = document[\"law_id\"]\n",
    "    for article in document[\"articles\"]:\n",
    "        article_id = article[\"article_id\"]\n",
    "        text = article[\"segment_only\"]\n",
    "        content.append(text)\n",
    "        index.append(law_id + \" \" + article_id)\n",
    "print(len(content))\n",
    "print(len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23797cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028c0f004cf74ecba29ba80dd050f8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61425, 768)\n"
     ]
    }
   ],
   "source": [
    "array = sentence_embedded_model.encode(content, batch_size=4, show_progress_bar=True)\n",
    "\n",
    "print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a8c834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11260\\3183747471.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "# 2. Kết nối Qdrant\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "client.recreate_collection(\n",
    "    collection_name=\"PhoBERT_Embedded_Law_Retrieval\",\n",
    "    vectors_config=VectorParams(size=array.shape[1], distance=Distance.COSINE) # Using Cosinesimilarity for searching vector\n",
    ")\n",
    "payloads = [{\"law_id\": index[i].split(\" \")[0], \"article_id\": index[i].split(\" \")[1], \"doc_id\": i} for i in range(len(index))]\n",
    "# points = [(i, vectors[i], payloads[i]) for i in range(len(documents))]\n",
    "points = [\n",
    "    {\n",
    "        \"id\": i,\n",
    "        \"vector\": array[i],\n",
    "        \"payload\": payloads[i]\n",
    "    }\n",
    "    for i in range(len(content))\n",
    "]\n",
    "for i in range(0, len(points), 100):\n",
    "    client.upsert(collection_name=\"PhoBERT_Embedded_Law_Retrieval\", points=points[i:i+100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6662f6",
   "metadata": {},
   "source": [
    "## **USING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe11efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41dec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vncoreNLP_model = py_vncorenlp.VnCoreNLP(save_dir= MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb5404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_query(query):\n",
    "    query_list = vncoreNLP_model.word_segment(query) # segment\n",
    "    cleaned_query = \" \".join(query_list)\n",
    "    return cleaned_query\n",
    "\n",
    "def vectorize_query(query):\n",
    "    cleaned_query = clean_query(query)\n",
    "    query_vector = sentence_embedded_model.encode(cleaned_query)\n",
    "    query_vector = normalize(query_vector.reshape(1, -1))[0]\n",
    "    return query_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbd08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7815 | law_id: 63/2020/nđ-cp | article_id: 20 \n",
      "Score: 0.7489 | law_id: 63/2020/nđ-cp | article_id: 19 \n",
      "Score: 0.7371 | law_id: 63/2020/nđ-cp | article_id: 1 \n",
      "Score: 0.6894 | law_id: 63/2020/nđ-cp | article_id: 21 \n",
      "Score: 0.6771 | law_id: 63/2020/nđ-cp | article_id: 4 \n",
      "Score: 0.6401 | law_id: 63/2020/nđ-cp | article_id: 14 \n",
      "Score: 0.6203 | law_id: 63/2020/nđ-cp | article_id: 18 \n",
      "Score: 0.6135 | law_id: 63/2020/nđ-cp | article_id: 7 \n",
      "Score: 0.6130 | law_id: 63/2020/nđ-cp | article_id: 9 \n",
      "Score: 0.5894 | law_id: 63/2020/nđ-cp | article_id: 10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11260\\1284670286.py:5: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    }
   ],
   "source": [
    "query = \"Trách nhiệm của Bộ Khoa học và Công nghệ về quản lý và phát triển công nghiệp an ninh được quy định như thế nào?\"\n",
    "\n",
    "query_vector = vectorize_query(query)\n",
    "\n",
    "hits = client.search(\n",
    "    collection_name=\"PhoBERT_Embedded_Law_Retrieval\",\n",
    "    query_vector=query_vector,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "for hit in hits:\n",
    "    print(f\"Score: {hit.score:.4f} | law_id: {hit.payload['law_id']} | article_id: {hit.payload['article_id']} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QandA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
